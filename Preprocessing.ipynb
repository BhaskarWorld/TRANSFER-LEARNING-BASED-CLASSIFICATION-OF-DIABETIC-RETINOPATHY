{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import time\n",
    "import xlrd \n",
    "import h5py\n",
    "import keras\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "import pydicom as dicom\n",
    "import matplotlib.pyplot as plot\n",
    "from ipywidgets import IntProgress\n",
    "from keras.models import Sequential\n",
    "from tqdm.notebook import tqdm as tq\n",
    "from keras.utils import to_categorical\n",
    "from skimage import measure,morphology,segmentation\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  find_horizontal_indexes(img,offset): \n",
    "    h_pixel_count = img.shape[1]\n",
    "    v_pixel_count = img.shape[0]\n",
    "    x_initial = 10000\n",
    "    x_final = 0\n",
    "    for j in range(-20,21):   \n",
    "        for i in range(h_pixel_count):\n",
    "            if(img[(v_pixel_count//2)+j][i][0]>offset and img[(v_pixel_count //2)+j][i][2]>offset and img[(v_pixel_count//2)+j][i][1]>offset):\n",
    "                if(x_initial>i):\n",
    "                    x_initial = i\n",
    "                break\n",
    "\n",
    "        for k in range(h_pixel_count):\n",
    "            zh=h_pixel_count-k-1\n",
    "            if(img[(v_pixel_count//2)+j][zh][0]>offset and img[(v_pixel_count//2)+j][zh][2]>offset and img[(v_pixel_count//2)+j][zh][1]>offset):\n",
    "                if(x_final<zh):\n",
    "                    x_final = zh\n",
    "                break\n",
    "    return (x_initial,x_final)\n",
    "\n",
    "\n",
    "\n",
    "def  find_vertical_indexes(img,offset): \n",
    "    h_pixel_count = img.shape[1]\n",
    "    v_pixel_count = img.shape[0]\n",
    "    y_initial = 1000000\n",
    "    y_final = 0\n",
    "    for j in range(-20,21):   \n",
    "        for i in range(v_pixel_count):\n",
    "            if(img[i][(h_pixel_count//2)+j][0]>offset and img[i][(h_pixel_count//2)+j][1]>offset and img[i][(h_pixel_count//2)+j][2]>offset):\n",
    "                if(y_initial>i):\n",
    "                    y_initial = i\n",
    "                break\n",
    "\n",
    "        for k in range(v_pixel_count):\n",
    "            zv=v_pixel_count-k-1\n",
    "            if(img[zv][(h_pixel_count//2)+j][0]>offset and img[zv][(h_pixel_count//2)+j][1]>offset and img[zv][(h_pixel_count//2)+j][2]>offset):\n",
    "                if(y_final<zv):\n",
    "                    y_final = zv\n",
    "                break\n",
    "    return (y_initial,y_final)\n",
    "\n",
    "\n",
    "\n",
    "def crop(img,offset):\n",
    "\n",
    "    x1,x2 = find_horizontal_indexes(img,offset)\n",
    "    y1,y2 = find_vertical_indexes(img,offset)\n",
    "    imgc = img[y1:y2,x1:x2,:]\n",
    "    \n",
    "    return imgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Data Augmentation\n",
    "def DataAugmentor(Image_path,Save_image_path,Image_name,Aug_per_image):\n",
    "    # Initialising the ImageDataGenerator class. \n",
    "    # We will pass in the augmentation parameters in the constructor. \n",
    "    datagen = ImageDataGenerator( \n",
    "                rotation_range=20,\n",
    "                vertical_flip = True,  \n",
    "                horizontal_flip = True,fill_mode ='constant',cval=0) \n",
    "\n",
    "    # Loading a sample image  \n",
    "    img = load_img(Image_path)  \n",
    "    # Converting the input sample image to an array \n",
    "    x = img_to_array(img) \n",
    "    # Reshaping the input image \n",
    "    x = x.reshape((1, ) + x.shape)  \n",
    "\n",
    "    # Generating and saving 5 augmented samples  \n",
    "    # using the above defined parameters.  \n",
    "    i = 0\n",
    "    for batch in datagen.flow(x, batch_size = 5, \n",
    "                              save_to_dir =Save_image_path,  \n",
    "                              save_prefix =Image_name, save_format ='jpeg'): \n",
    "        i+=1\n",
    "        if (i>=Aug_per_image):  \n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grey_clahe(img,clipLimitVal,tileGridSizeVal):\n",
    "    # Method to apply CLAHE filter to the image (More Effective)\n",
    "    img_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    obj_clahe = cv2.createCLAHE(clipLimit=clipLimitVal, tileGridSize=(tileGridSizeVal))\n",
    "    output = obj_clahe.apply(img_grey)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def maxmin(img,minVal,maxVal):\n",
    "    # Method to apply Maxmin to the image \n",
    "    newmin = minVal\n",
    "    newmax = maxVal\n",
    "    img = newmin+(img - img.min())*(newmax - newmin)/(img.max()-img.min())\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the label provided in CSV file and  \n",
    "# splitting the images to different folders\n",
    "# based on thier class mentioned in the label\n",
    "filename = \"C:/Train/trainLabels.csv\"\n",
    "rows = [] \n",
    "  \n",
    "# reading csv file \n",
    "with open(filename, 'r') as csvfile: \n",
    "    csvreader = csv.reader(csvfile) \n",
    "    for row in csvreader: \n",
    "        rows.append(row) \n",
    "\n",
    "\n",
    "# Splitting\n",
    "for row in tq(rows):\n",
    "    if(row[1]=='0'):\n",
    "        shutil.move('C:/Train/train/'+row[0]+'.jpeg','C:/project/CategorizedData/class_0/'+row[0]+'.jpeg')\n",
    "    elif(row[1]=='1'):\n",
    "        shutil.move('C:/Train/train/'+row[0]+'.jpeg','C:/project/CategorizedData/class_1/'+row[0]+'.jpeg')\n",
    "    elif(row[1]=='2'):\n",
    "        shutil.move('C:/Train/train/'+row[0]+'.jpeg','C:/project/CategorizedData/class_2/'+row[0]+'.jpeg')\n",
    "    elif(row[1]=='3'):\n",
    "        shutil.move('C:/Train/train/'+row[0]+'.jpeg','C:/project/CategorizedData/class_3/'+row[0]+'.jpeg')\n",
    "    elif(row[1]=='4'):\n",
    "        shutil.move('C:/Train/train/'+row[0]+'.jpeg','C:/project/CategorizedData/class_4/'+row[0]+'.jpeg')\n",
    "        \n",
    "print(\"Transfer Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section of code is responsible for cropping the images\n",
    "Input_path = \"C:/Project/CategorizedData/\"\n",
    "classes = os.listdir(Input_path)\n",
    "for clas in tq(classes,\"progress\"):\n",
    "    Image_path = Input_path+clas\n",
    "    \n",
    "    Save_image_path = \"C:/Project/CroppedData/\"+clas+\"/\"\n",
    "    for images  in tq(os.listdir(Image_path),clas,leave=True):\n",
    "        name,ext = images.split(\".\")\n",
    "        im = cv2.imread(os.path.join(Image_path,images))\n",
    "        imc = crop(im,4)\n",
    "        cv2.imwrite(Save_image_path+images,imc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This section of code is responsible for Augmenting the images\n",
    "# The aim is to create atleast 10000 images in total for each class\n",
    "# Classes which have more than 10000 images will be skipped \n",
    "# and which does not have 10000 images then this method will calculate the no of images needed to have 10000 image \n",
    "# and accordingly performs number of augmentation per image \n",
    "Input_path = \"C:/Project/CroppedData/\"\n",
    "classes = os.listdir(Input_path)\n",
    "for clas in tq(classes[1:],\"progress\"):\n",
    "    Image_path = Input_path+clas\n",
    "    Augperimg = int(np.ceil((10000-len(os.listdir(Image_path))/len(os.listdir(Image_path))))\n",
    "    Save_image_path = \"C:/Project/AugmentedData/\"+clas+\"/\"\n",
    "    print(Augperimg)\n",
    "    for images  in tq(os.listdir(Image_path),clas,leave=True):\n",
    "                    \n",
    "                    \n",
    "        name,ext = images.split(\".\")\n",
    "        \n",
    "        DataAugmentor(os.path.join(Image_path,images),Save_image_path,name,Augperimg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the random 10000 images are collected \n",
    "#from each class for further preprocessing\n",
    "\n",
    "Source_path = \"C:/Project/AugmentedData/\"\n",
    "Destination_path = \"C:/Project/Data_10000/\"\n",
    "\n",
    "for classes in os.listdir(Source_path):\n",
    "    image_list = os.listdir(Source_path+classes)\n",
    "    img_move = random.sample(image_list,10000)\n",
    "    for image in tq(img_move,\"Progress\"):\n",
    "        shutil.move(os.path.join(Source_path,classes,image),os.path.join(Destination_path,classes,image)) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those 10000 images from each classes \n",
    "# are now going to get preprocessed in this section\n",
    "\n",
    "Source_path = \"C:/Project/Data_10000/\"\n",
    "Destination_path = \"C:/Project/PreprossedData/\"\n",
    "clipLimitVal = 8\n",
    "tileGridSizeVal = (8,8)\n",
    "sigmaX = 30\n",
    "size = (224,224)\n",
    "for classes in tq(os.listdir(Source_path),\"Progress:\"):\n",
    "    image_list = os.listdir(Source_path+classes)\n",
    "    for images in tq( image_list, classes, leave= False):\n",
    "        img = cv2.imread(os.path.join(Source_path,classes,images))\n",
    "        try:\n",
    "            image = cv2.resize(img, size)\n",
    "        except:\n",
    "            \n",
    "            print(images)\n",
    "            continue\n",
    "        image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n",
    "        cv2.imwrite(os.path.join(Destination_path,classes,images),image)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed data are again resized in this section\n",
    "Source_path = \"C:/Project/PreprossedData/\"\n",
    "Destination_path = \"C:/Project/ResizedData/\"\n",
    "minVal = 0\n",
    "maxVal= 1\n",
    "size = (224,224)\n",
    "\n",
    "for classes in tq(os.listdir(Source_path),\"Progress:\"):\n",
    "    image_list = os.listdir(Source_path+classes)\n",
    "    for image in tq( image_list, classes, leave= False):\n",
    "        img = cv2.imread(os.path.join(Source_path,classes,image))\n",
    "        imgr = cv2.resize(img,(size))\n",
    "        imgm = maxmin(imgr,minVal,maxVal)\n",
    "        cv2.imwrite(os.path.join(Destination_path,classes,image),imgm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the Preprocessed Data into Training Set,Testing Set and Validation Set\n",
    "\n",
    "Source_path = 'C:/Project/PreprossedData/'\n",
    "Destination_path = 'D:/prog_data/'\n",
    "\n",
    "for classes in tq(os.listdir(Source_path),\"progress\"):\n",
    "    path_source = os.path.join(Source_path,classes)\n",
    "    images = os.listdir(path_source)\n",
    "    size=len(images)\n",
    "    train=int(.85*size)\n",
    "    val= train+ int(.10*size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    random.shuffle(images)\n",
    "    \n",
    "    \n",
    "    path_train_destination = os.path.join(Destination_path,\"training\",classes)\n",
    "    path_test_destination = os.path.join(Destination_path,\"testing\",classes)\n",
    "    path_val_destination = os.path.join(Destination_path,\"validation\",classes)\n",
    "    for img in tq(images[0:train],\"training\", leave = False):\n",
    "        shutil.move(os.path.join(path_source,img), os.path.join(path_train_destination,img))\n",
    "        \n",
    "    for img in tq(images[train:val],\"testing\", leave = False):\n",
    "        shutil.move(os.path.join(path_source,img), os.path.join(path_val_destination,img))\n",
    "        \n",
    "    for img in tq(images[val:size],\"validation\",leave = False):\n",
    "        shutil.move(os.path.join(path_source,img), os.path.join(path_test_destination,img))\n",
    "    print(classes,\"moved.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
